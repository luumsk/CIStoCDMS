{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the sys.path to avoid 'ModuleNotFoundError'\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import friedmanchisquare\n",
    "from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"font.family\"] = ['serif']\n",
    "\n",
    "from src.helpers import load_json, load_pickle\n",
    "from src.paths import paths\n",
    "from src.config import MODEL_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check if metrics have normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = load_json(paths.get('metric_fold_path'))\n",
    "metrics_to_analyze = [\n",
    "    'accuracy',\n",
    "    'roc_auc',\n",
    "    'f1_score',\n",
    "    'precision',\n",
    "    'recall',\n",
    "    'specificity'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>metric</th>\n",
       "      <th>statistic</th>\n",
       "      <th>p_value</th>\n",
       "      <th>normality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>catboost</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.946522</td>\n",
       "      <td>0.712336</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>catboost</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.838784</td>\n",
       "      <td>0.161588</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>catboost</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.980866</td>\n",
       "      <td>0.939200</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>catboost</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.940116</td>\n",
       "      <td>0.666758</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>catboost</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>0.085693</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>catboost</td>\n",
       "      <td>specificity</td>\n",
       "      <td>0.910757</td>\n",
       "      <td>0.472151</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.977634</td>\n",
       "      <td>0.921584</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.800256</td>\n",
       "      <td>0.081425</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.963147</td>\n",
       "      <td>0.829707</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.946566</td>\n",
       "      <td>0.712645</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.805882</td>\n",
       "      <td>0.090412</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>specificity</td>\n",
       "      <td>0.972904</td>\n",
       "      <td>0.893546</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.914506</td>\n",
       "      <td>0.495137</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.845051</td>\n",
       "      <td>0.179374</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.875131</td>\n",
       "      <td>0.287826</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.992925</td>\n",
       "      <td>0.988868</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.754427</td>\n",
       "      <td>0.032692</td>\n",
       "      <td>not normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>specificity</td>\n",
       "      <td>0.921899</td>\n",
       "      <td>0.542261</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rf</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.967536</td>\n",
       "      <td>0.859217</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rf</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.966164</td>\n",
       "      <td>0.850109</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rf</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.985422</td>\n",
       "      <td>0.961354</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rf</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.875911</td>\n",
       "      <td>0.291190</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rf</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.952351</td>\n",
       "      <td>0.753973</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rf</td>\n",
       "      <td>specificity</td>\n",
       "      <td>0.643807</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>not normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>svm</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.928761</td>\n",
       "      <td>0.587945</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>svm</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.996844</td>\n",
       "      <td>0.997307</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>svm</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.951760</td>\n",
       "      <td>0.749757</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>svm</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.881378</td>\n",
       "      <td>0.315601</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>svm</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.952960</td>\n",
       "      <td>0.758312</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>svm</td>\n",
       "      <td>specificity</td>\n",
       "      <td>0.824689</td>\n",
       "      <td>0.126842</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lr</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.958475</td>\n",
       "      <td>0.797327</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>lr</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.898395</td>\n",
       "      <td>0.401077</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lr</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.982897</td>\n",
       "      <td>0.949499</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>lr</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.972488</td>\n",
       "      <td>0.890971</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lr</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>0.085693</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>lr</td>\n",
       "      <td>specificity</td>\n",
       "      <td>0.936310</td>\n",
       "      <td>0.639985</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model       metric  statistic   p_value   normality\n",
       "0   catboost     accuracy   0.946522  0.712336      normal\n",
       "1   catboost      roc_auc   0.838784  0.161588      normal\n",
       "2   catboost     f1_score   0.980866  0.939200      normal\n",
       "3   catboost    precision   0.940116  0.666758      normal\n",
       "4   catboost       recall   0.802990  0.085693      normal\n",
       "5   catboost  specificity   0.910757  0.472151      normal\n",
       "6    xgboost     accuracy   0.977634  0.921584      normal\n",
       "7    xgboost      roc_auc   0.800256  0.081425      normal\n",
       "8    xgboost     f1_score   0.963147  0.829707      normal\n",
       "9    xgboost    precision   0.946566  0.712645      normal\n",
       "10   xgboost       recall   0.805882  0.090412      normal\n",
       "11   xgboost  specificity   0.972904  0.893546      normal\n",
       "12      lgbm     accuracy   0.914506  0.495137      normal\n",
       "13      lgbm      roc_auc   0.845051  0.179374      normal\n",
       "14      lgbm     f1_score   0.875131  0.287826      normal\n",
       "15      lgbm    precision   0.992925  0.988868      normal\n",
       "16      lgbm       recall   0.754427  0.032692  not normal\n",
       "17      lgbm  specificity   0.921899  0.542261      normal\n",
       "18        rf     accuracy   0.967536  0.859217      normal\n",
       "19        rf      roc_auc   0.966164  0.850109      normal\n",
       "20        rf     f1_score   0.985422  0.961354      normal\n",
       "21        rf    precision   0.875911  0.291190      normal\n",
       "22        rf       recall   0.952351  0.753973      normal\n",
       "23        rf  specificity   0.643807  0.002247  not normal\n",
       "24       svm     accuracy   0.928761  0.587945      normal\n",
       "25       svm      roc_auc   0.996844  0.997307      normal\n",
       "26       svm     f1_score   0.951760  0.749757      normal\n",
       "27       svm    precision   0.881378  0.315601      normal\n",
       "28       svm       recall   0.952960  0.758312      normal\n",
       "29       svm  specificity   0.824689  0.126842      normal\n",
       "30        lr     accuracy   0.958475  0.797327      normal\n",
       "31        lr      roc_auc   0.898395  0.401077      normal\n",
       "32        lr     f1_score   0.982897  0.949499      normal\n",
       "33        lr    precision   0.972488  0.890971      normal\n",
       "34        lr       recall   0.802990  0.085693      normal\n",
       "35        lr  specificity   0.936310  0.639985      normal"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_normality(metrics_dict, metrics_to_analyze):\n",
    "    results = []\n",
    "\n",
    "    for model, metrics_list in metrics_dict.items():\n",
    "        for metric in metrics_to_analyze:\n",
    "            values = [metrics[metric] for metrics in metrics_list]\n",
    "            stat, p_value = stats.shapiro(values)\n",
    "            result = {\n",
    "                'model': model,\n",
    "                'metric': metric,\n",
    "                'statistic': stat,\n",
    "                'p_value': p_value,\n",
    "                'normality': 'normal' if p_value > 0.05 else 'not normal'\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "normality_results_df = check_normality(metrics_dict, metrics_to_analyze)\n",
    "normality_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Comparing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metric_scores(metrics_dict, metric_name):\n",
    "    metric_scores = {}\n",
    "    for model_name in MODEL_NAMES:\n",
    "        metric_scores[model_name] = [score[metric_name] for score in metrics_dict[model_name]]\n",
    "    return metric_scores\n",
    "\n",
    "def perform_friedman_test(metrics_dict, metric_name, verbose=0):\n",
    "    # Prepare the data in the format required for the Friedman test\n",
    "    metric_scores = extract_metric_scores(metrics_dict, metric_name)\n",
    "    scores = [metric_scores[model_name] for model_name in metric_scores]\n",
    "    \n",
    "    # Perform the Friedman test\n",
    "    stat, p_value = friedmanchisquare(*scores)\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Friedman test result for {metric_name}:\")\n",
    "    if verbose:\n",
    "        print(f\"Test Statistic: {stat}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"Significant differences\")\n",
    "    else:\n",
    "        print(f\"No significant differences\")\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test result for accuracy:\n",
      "Significant differences\n",
      "\n",
      "Friedman test result for roc_auc:\n",
      "Significant differences\n",
      "\n",
      "Friedman test result for f1_score:\n",
      "Significant differences\n",
      "\n",
      "Friedman test result for precision:\n",
      "Significant differences\n",
      "\n",
      "Friedman test result for recall:\n",
      "No significant differences\n",
      "\n",
      "Friedman test result for specificity:\n",
      "Significant differences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_to_analyze = ['accuracy', 'roc_auc', 'f1_score', 'precision', 'recall', 'specificity']\n",
    "\n",
    "for metric_name in metrics_to_analyze:\n",
    "    perform_friedman_test(metrics_dict, metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_different_models(metrics_dict, metric_name, verbose=0):\n",
    "    metric_scores = extract_metric_scores(metrics_dict, metric_name)\n",
    "    scores = np.array([metric_scores[model_name] for model_name in metric_scores])\n",
    "            \n",
    "    # Perform the Nemenyi post-hoc test\n",
    "    nemenyi_results = posthoc_nemenyi_friedman(scores.T)\n",
    "    \n",
    "    print(f\"\\nNemenyi post-hoc test results for {metric_name}:\")\n",
    "    if verbose:\n",
    "        print(nemenyi_results)\n",
    "    \n",
    "    significant_pairs = np.where(nemenyi_results < 0.05)\n",
    "    for i in range(len(significant_pairs[0])):\n",
    "        model1 = list(metric_scores.keys())[significant_pairs[0][i]]\n",
    "        model2 = list(metric_scores.keys())[significant_pairs[1][i]]\n",
    "        print(f\"Nemenyi post-hoc test found significant differences between {model1} and {model2}\")\n",
    "        \n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nemenyi post-hoc test results for accuracy:\n",
      "Nemenyi post-hoc test found significant differences between lgbm and lr\n",
      "Nemenyi post-hoc test found significant differences between lr and lgbm\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nemenyi post-hoc test results for roc_auc:\n",
      "Nemenyi post-hoc test found significant differences between catboost and lr\n",
      "Nemenyi post-hoc test found significant differences between lr and catboost\n",
      "\n",
      "\n",
      "Nemenyi post-hoc test results for f1_score:\n",
      "Nemenyi post-hoc test found significant differences between lgbm and lr\n",
      "Nemenyi post-hoc test found significant differences between lr and lgbm\n",
      "\n",
      "\n",
      "Nemenyi post-hoc test results for precision:\n",
      "Nemenyi post-hoc test found significant differences between lgbm and svm\n",
      "Nemenyi post-hoc test found significant differences between svm and lgbm\n",
      "\n",
      "\n",
      "Nemenyi post-hoc test results for specificity:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_to_analyze = ['accuracy', 'roc_auc', 'f1_score', 'precision', 'specificity']\n",
    "\n",
    "for metric_name in metrics_to_analyze:\n",
    "    identify_different_models(metrics_dict, metric_name, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Comparing the Impact of Features Using SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 5 (54, 20)\n"
     ]
    }
   ],
   "source": [
    "shap_values = load_pickle(paths.get('shap_values_path'))\n",
    "print(len(shap_values), len(shap_values['catboost']), shap_values['catboost'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIStoCDMS-fgf1c-B4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
